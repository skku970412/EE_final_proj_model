{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e1bf70",
   "metadata": {},
   "source": [
    "# YOLOv8 + Tesseract OCR 노트북\n",
    "\n",
    "이 노트북은 현재 디렉터리(`/home/work/llama_young/차량번호판인식_yolov8`) 내에서 YOLOv8 탐지 모델과 Tesseract OCR을 이용해 단일 이미지 또는 스트림의 번호판 문자열을 추론합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a34177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가상환경 경로 설정 완료: /home/work/llama_young/차량번호판인식_yolov8/venv\n",
      "현재 커널 파이썬: /usr/bin/python\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 가상환경을 노트북 커널에 반영합니다.\n",
    "VENV_DIR = Path('venv')\n",
    "if not VENV_DIR.exists():\n",
    "    raise FileNotFoundError(f'가상환경을 찾을 수 없습니다: {VENV_DIR.resolve()}')\n",
    "\n",
    "site_packages = sorted((VENV_DIR / 'lib').glob('python*/site-packages'))\n",
    "if not site_packages:\n",
    "    raise FileNotFoundError('site-packages 디렉터리가 없습니다. 가상환경 구성을 확인하세요.')\n",
    "site_packages_path = site_packages[0].resolve()\n",
    "if str(site_packages_path) not in sys.path:\n",
    "    sys.path.insert(0, str(site_packages_path))\n",
    "\n",
    "os.environ['VIRTUAL_ENV'] = str(VENV_DIR.resolve())\n",
    "bin_path = (VENV_DIR / 'bin').resolve()\n",
    "current_path = os.environ.get('PATH', '')\n",
    "if current_path:\n",
    "    parts = current_path.split(':')\n",
    "    if str(bin_path) not in parts:\n",
    "        os.environ['PATH'] = f\"{bin_path}:{current_path}\"\n",
    "else:\n",
    "    os.environ['PATH'] = str(bin_path)\n",
    "\n",
    "print('가상환경 경로 설정 완료:', VENV_DIR.resolve())\n",
    "print('현재 커널 파이썬:', sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f514d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA 사용 가능 여부: True\n",
      "OpenCV version: 4.12.0\n",
      "Tesseract version: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import pytesseract\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA 사용 가능 여부:', torch.cuda.is_available())\n",
    "print('OpenCV version:', cv2.__version__)\n",
    "print('Tesseract version:', pytesseract.get_tesseract_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa90906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 디렉터리: /home/work/llama_young/차량번호판인식_yolov8/runs/ocr_demo\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- 사용자 설정 ---\n",
    "DET_WEIGHTS = Path('runs/license_plate_yolov8n/weights/best.pt')  # YOLO 탐지 모델 가중치\n",
    "INPUT_PATH = Path('samples/example.jpg')  # 처리할 이미지/동영상 경로 또는 '0' 문자열(웹캠)\n",
    "OUTPUT_DIR = Path('runs/ocr_demo')\n",
    "CONF_THRES = 0.5\n",
    "IOU_THRES = 0.65\n",
    "IMGSZ = 512\n",
    "USE_HALF = True  # GPU 사용 시 True 권장\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('출력 디렉터리:', OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5151a6",
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pytesseract\nimport torch\nfrom ultralytics import YOLO\nimport easyocr\n\nPLATE_RE = re.compile(r\"\\b\\d{2,3}[가-힣]-?\\d{4}\\b\")\nSUBS = str.maketrans({'O': '0', 'o': '0', 'I': '1', 'l': '1', 'S': '5', 'B': '8', 'D': '0', '—': '-'})\n\nDEBUG_SAVE = False\n\nEASY_OCR_READER = None\n\ndef get_easyocr_reader():\n    global EASY_OCR_READER\n    if EASY_OCR_READER is None:\n        EASY_OCR_READER = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())\n    return EASY_OCR_READER\n\n\ndef order_points(pts: np.ndarray) -> np.ndarray:\n    rect = np.zeros((4, 2), dtype=np.float32)\n    s = pts.sum(axis=1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    diff = np.diff(pts, axis=1).reshape(-1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect\n\ndef extract_plate_patch(roi: np.ndarray) -> np.ndarray:\n    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n    edges = cv2.Canny(blur, 50, 150)\n    edges = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=1)\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    area = roi.shape[0] * roi.shape[1]\n    for cnt in sorted(contours, key=cv2.contourArea, reverse=True):\n        x, y, w, h = cv2.boundingRect(cnt)\n        if h <= 0:\n            continue\n        ar = w / h\n        coverage = (w * h) / max(1, area)\n        if 1.5 <= ar <= 8.0 and coverage > 0.08:\n            peri = cv2.arcLength(cnt, True)\n            approx = cv2.approxPolyDP(cnt, 0.03 * peri, True)\n            if len(approx) == 4:\n                pts = approx.reshape(4, 2).astype(np.float32)\n                rect = order_points(pts)\n                W = int(max(np.linalg.norm(rect[0] - rect[1]), np.linalg.norm(rect[2] - rect[3])))\n                H = int(max(np.linalg.norm(rect[0] - rect[3]), np.linalg.norm(rect[1] - rect[2])))\n                W = max(W, 320); H = max(H, 80)\n                dst = np.array([[0, 0], [W - 1, 0], [W - 1, H - 1], [0, H - 1]], dtype=np.float32)\n                M = cv2.getPerspectiveTransform(rect, dst)\n                return cv2.warpPerspective(roi, M, (W, H))\n            return roi[y : y + h, x : x + w]\n    return roi\n\ndef preprocess_variants(roi: np.ndarray) -> list[np.ndarray]:\n    roi = extract_plate_patch(roi)\n    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n    gray = cv2.resize(gray, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n    gray = cv2.bilateralFilter(gray, 9, 75, 75)\n    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n    eq = clahe.apply(gray)\n    _, otsu = cv2.threshold(eq, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    otsu_inv = cv2.bitwise_not(otsu)\n    gauss = cv2.adaptiveThreshold(eq, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 3)\n    gauss_inv = cv2.bitwise_not(gauss)\n    morph = cv2.morphologyEx(gauss, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8), iterations=2)\n    return [eq, otsu, otsu_inv, gauss, gauss_inv, morph]\n\ndef score_text(text: str) -> tuple[int, int]:\n    match = PLATE_RE.search(text)\n    return (2 if match else 0, len(text))\n\ndef ocr_plate(roi: np.ndarray) -> tuple[str, np.ndarray]:\n    variants = preprocess_variants(roi)\n    configs = [\n        \"--oem 1 --psm 7 -l kor+eng -c tessedit_char_whitelist=0123456789가-힣-\",\n        \"--oem 1 --psm 8 -l kor+eng -c tessedit_char_whitelist=0123456789가-힣-\",\n        \"--oem 3 --psm 6 -l kor+eng -c tessedit_char_whitelist=0123456789가-힣-\",\n        \"--oem 3 --psm 11 -l kor+eng -c tessedit_char_whitelist=0123456789가-힣-\",\n    ]\n    best_text, best_img, best_score = '', variants[0], (-1, -1)\n    candidate_texts: set[str] = set()\n\n    for idx, variant in enumerate(variants):\n        for cfg in configs:\n            raw = pytesseract.image_to_string(variant, config=cfg)\n            cleaned = re.sub(r\"[^0-9가-힣-]\", '', raw.translate(SUBS))\n            if cleaned:\n                candidate_texts.add(cleaned)\n            score = score_text(cleaned)\n            if score > best_score:\n                best_text, best_img, best_score = cleaned, variant, score\n                if DEBUG_SAVE:\n                    cv2.imwrite(f\"debug_variant_{idx}.png\", variant)\n\n    if best_score[0] < 2:\n        reader = get_easyocr_reader()\n        for idx, variant in enumerate(variants):\n            results = reader.readtext(variant, detail=1, paragraph=False)\n            results = sorted(results, key=lambda r: min(pt[0] for pt in r[0]))\n            merged = \"\"\n            for bbox, raw, conf in results:\n                cleaned = re.sub(r\"[^0-9가-힣-]\", \"\", raw.translate(SUBS))\n                if cleaned:\n                    candidate_texts.add(cleaned)\n                    merged += cleaned\n                    score = score_text(cleaned)\n                    if score > best_score:\n                        best_text, best_img, best_score = cleaned, variant, score\n            if merged:\n                candidate_texts.add(merged)\n                score = score_text(merged)\n                if score > best_score:\n                    best_text, best_img, best_score = merged, variant, score\n    if candidate_texts:\n        matches = set()\n        for text in candidate_texts:\n            matches.update(PLATE_RE.findall(text))\n        if best_text:\n            matches.update(PLATE_RE.findall(best_text))\n        if matches:\n            best_candidate = max(matches, key=lambda t: score_text(t))\n            return best_candidate.replace('-', ''), best_img\n\n    return best_text.replace('-', ''), best_img\ndef aspect_ratio_ok(x1: int, y1: int, x2: int, y2: int, min_area: int = 2000) -> bool:\n    w, h = x2 - x1, y2 - y1\n    if h <= 0:\n        return False\n    ar = w / h\n    return (1.5 <= ar <= 8.0) and (w * h >= min_area)\n\ndef annotate(frame: np.ndarray, box, label: str | None, color=(0, 255, 0)) -> None:\n    x1, y1, x2, y2 = map(int, box)\n    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n    if label:\n        cv2.putText(frame, label, (x1, max(20, y1 - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n\ndef load_model(path: Path) -> YOLO:\n    if not path.exists():\n        raise FileNotFoundError(f'탐지 가중치를 찾을 수 없습니다: {path}')\n    return YOLO(str(path))"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadc01c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "탐지 모델 로드 완료. device: cuda\n",
      "저장 완료: runs/ocr_demo/example.jpg\n"
     ]
    }
   ],
   "source": [
    "model = load_model(DET_WEIGHTS)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('탐지 모델 로드 완료. device:', device)\n",
    "\n",
    "source_str = str(INPUT_PATH)\n",
    "is_webcam = source_str.isdigit() and not Path(source_str).exists()\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    start = time.time()\n",
    "    res = model.predict(\n",
    "        frame,\n",
    "        imgsz=IMGSZ,\n",
    "        conf=CONF_THRES,\n",
    "        iou=IOU_THRES,\n",
    "        device=device,\n",
    "        agnostic_nms=True,\n",
    "        max_det=2,\n",
    "        half=USE_HALF and device != 'cpu',\n",
    "        verbose=False,\n",
    "    )[0]\n",
    "    picks = []\n",
    "    for box, score in zip(res.boxes.xyxy.cpu().numpy(), res.boxes.conf.cpu().numpy()):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        if aspect_ratio_ok(x1, y1, x2, y2):\n",
    "            picks.append((float(score), (x1, y1, x2, y2)))\n",
    "    picks.sort(reverse=True)\n",
    "\n",
    "    plate_text = ''\n",
    "    if picks:\n",
    "        _, (x1, y1, x2, y2) = picks[0]\n",
    "        h, w = frame.shape[:2]\n",
    "        pad = int(0.1 * max(x2 - x1, y2 - y1))\n",
    "        x1, y1 = max(0, x1 - pad), max(0, y1 - pad)\n",
    "        x2, y2 = min(w, x2 + pad), min(h, y2 + pad)\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "        if roi.size > 0:\n",
    "            plate_text, _ = ocr_plate(roi)\n",
    "        annotate(frame, (x1, y1, x2, y2), plate_text or 'reading...', (0, 255, 0))\n",
    "    fps = 1.0 / max(1e-6, time.time() - start)\n",
    "    cv2.putText(frame, f\"{fps:.1f} FPS\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)\n",
    "    return frame, plate_text\n",
    "\n",
    "if is_webcam:\n",
    "    import cv2\n",
    "\n",
    "    cap = cv2.VideoCapture(int(source_str))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError('웹캠을 열 수 없습니다.')\n",
    "    print('웹캠 스트림 시작. 종료하려면 창에서 q 키를 누르세요.')\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        vis, plate = process_frame(frame)\n",
    "        cv2.imshow('YOLO + Tesseract', vis)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    ext = INPUT_PATH.suffix.lower()\n",
    "    if ext in {'.jpg', '.jpeg', '.png', '.bmp'}:\n",
    "        frame = cv2.imread(str(INPUT_PATH))\n",
    "        if frame is None:\n",
    "            raise FileNotFoundError(f'이미지를 불러올 수 없습니다: {INPUT_PATH}')\n",
    "        vis, plate = process_frame(frame)\n",
    "        out_path = OUTPUT_DIR / INPUT_PATH.name\n",
    "        cv2.imwrite(str(out_path), vis)\n",
    "        print('저장 완료:', out_path)\n",
    "        if plate:\n",
    "            print('인식 결과:', plate)\n",
    "    else:\n",
    "        for res in model.predict(\n",
    "            source=str(INPUT_PATH),\n",
    "            imgsz=IMGSZ,\n",
    "            conf=CONF_THRES,\n",
    "            iou=IOU_THRES,\n",
    "            device=device,\n",
    "            show=False,\n",
    "            half=USE_HALF and device != 'cpu',\n",
    "            stream=True,\n",
    "        ):\n",
    "            frame = res.orig_img\n",
    "            vis, plate = process_frame(frame)\n",
    "            out_path = OUTPUT_DIR / f\"frame_{res.frame_id:06d}.jpg\"\n",
    "            cv2.imwrite(str(out_path), vis)\n",
    "            if plate:\n",
    "                print(f\"frame {res.frame_id}: {plate}\")\n",
    "        print('동영상 처리가 완료되었습니다.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}